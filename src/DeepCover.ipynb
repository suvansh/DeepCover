{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import itertools\n",
    "from os.path import join\n",
    "from utils import get_repo_dir, datetime_str, logger, jsonify, sweep\n",
    "from config.config import *\n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Add yard line and orientation features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial variables and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = 30  # truncate if snap-to-throw is > this. units: ds\n",
    "num_features = 11  # number of channels in input (vx, vy, etc)\n",
    "num_extras = 1  # number of (play-level) features added in to outer network (just LoS for now)\n",
    "out_dir = join(get_repo_dir(), 'output')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "exps_dir = join(out_dir, 'exps')\n",
    "os.makedirs(exps_dir, exist_ok=True)\n",
    "\n",
    "\"\"\"\n",
    "Game Blacklist.\n",
    "2018092301: Tony Jefferson data bad\n",
    "\"\"\"\n",
    "game_blacklist = [2018092301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data_path = join(data_dir, 'full_data.pkl')\n",
    "if not os.path.isfile(full_data_path):\n",
    "    weeks = range(1, 18)\n",
    "    tracking_data = pd.concat([pd.read_csv(join(data_dir, f'week{week_num}_norm.csv')) for week_num in tqdm(weeks)], axis=0)\n",
    "    coverage_data = pd.read_csv(join(data_dir, 'coverages_2018.csv')).dropna(subset=['coverage'])\n",
    "    coverage_data.coverage.replace({'3 Seam': 'Cover 3 Zone', 'Cover 1 Double': 'Cover 1 Man'}, inplace=True)\n",
    "    coverage_data = coverage_data[~coverage_data.coverage.isin(['Bracket', 'Mis'])]\n",
    "    coverage_data.coverage = coverage_data.coverage.astype('category')\n",
    "    full_data = pd.merge(tracking_data, coverage_data, how='inner', left_on=['gameId', 'playId'], right_on=['game_id', 'play_id'])\n",
    "    full_data['coverage_code'] = full_data.coverage.cat.codes\n",
    "    # save full data\n",
    "    full_data.reset_index(drop=True).to_pickle(full_data_path)\n",
    "    \n",
    "    del tracking_data\n",
    "    gc.collect()\n",
    "else:\n",
    "    # load full data from previously saved zip\n",
    "    full_data = pd.read_pickle(full_data_path)\n",
    "coverage_label_map = dict(zip(full_data['coverage'].cat.codes, full_data['coverage']))\n",
    "num_classes = len(coverage_label_map)\n",
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO data aug. this is very slightly complicated by the fact that we have precomputed the vx, vy, ax, ay. will have to ask udit about how this is computed since it seems to be inconsistent with v_theta and v_mag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we don't have orientation relative to qb, so I don't think it makes sense to highlight the qb in the way the bdb winner does for rusher, since it would just be distance from qb and qb-relative speed/accel data right now, which I doubt is _extra_ useful for predicting coverage (?). so for now, we'll do 11x11 of off x def with features:\n",
    "* relative: x, y, vx, vy, ax, ay\n",
    "* absolute: vx, vy, ax, ay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    grouped = full_data.groupby(['gameId', 'playId'])\n",
    "    # TODO can change 11s to max number of off and def players in data\n",
    "    data_X = np.empty((len(grouped), max_num_frames, num_features, 11, 11), dtype=np.float32)  # (P, T, F, D, O): play, frame, feature, def, off\n",
    "    data_dims = np.empty((len(grouped), 3), dtype=np.int32) # (P, 3) contains (t, d, o): num frames, num def, num off on each play\n",
    "    data_extras = np.empty((len(grouped), num_extras), dtype=np.float32)  # just LoS for now\n",
    "    data_Y = np.empty(len(grouped), dtype=np.int32)  # (P,)\n",
    "    \n",
    "    valid_plays = 0\n",
    "    for (game_id, play_id), play_df in tqdm(grouped):\n",
    "        if game_id in game_blacklist:\n",
    "            continue\n",
    "        try:\n",
    "            first_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'ball_snap')].frameId.iloc[0]\n",
    "            play_end_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event.isin(['pass_forward', 'pass_shovel', 'qb_sack', 'qb_strip_sack', 'tackle']))].frameId.iloc[0]\n",
    "        except:\n",
    "            print(f'({game_id}, {play_id}) failed. events were {play_df.event.unique()}')\n",
    "            continue\n",
    "        last_frame = min(first_frame + max_num_frames - 1, play_end_frame)\n",
    "        play_df = play_df.loc[play_df.frameId.between(first_frame, last_frame)]\n",
    "\n",
    "        num_def, num_off = 0, 0\n",
    "        bad_play = False\n",
    "        for frame_idx, (frame_id, frame_df) in enumerate(play_df.groupby('frameId')):\n",
    "            def_ids = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "            num_def = len(def_ids)\n",
    "            off_ids = frame_df[frame_df.team_pos == 'OFF'].index\n",
    "            num_off = len(off_ids)\n",
    "            qb_id = frame_df\n",
    "            if num_def > 11 or num_off > 11:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                bad_play = True\n",
    "                break\n",
    "            if num_def < 3 or num_off < 3:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                bad_play = True\n",
    "                break\n",
    "            if (num_qbs := frame_df.loc[frame_df.position == 'QB'].shape[0]) != 1:\n",
    "                print(f'({game_id}, {play_id}), num QBs {num_qbs}')\n",
    "                bad_play = True\n",
    "                break\n",
    "\n",
    "            outer_sub = np.subtract.outer(\n",
    "                frame_df.loc[off_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values,\n",
    "                frame_df.loc[def_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values\n",
    "            )\n",
    "            if np.isnan(outer_sub).any():\n",
    "                print(f'({game_id}, {play_id}), frame {frame_id} has NaNs')\n",
    "                break\n",
    "            # einsum explanation: the two i's get rid of subtraction across cols\n",
    "            # k before j reorders def before off since output dims in alph. order\n",
    "            data_X[valid_plays, frame_idx, :6, :num_def, :num_off] = np.einsum('kiji->ijk', outer_sub)\n",
    "            data_X[valid_plays, frame_idx, 6:10, :num_def, :num_off] = frame_df.loc[def_ids, ['v_x', 'v_y', 'a_x', 'a_y']].values.T[...,None]\n",
    "            data_X[valid_plays, frame_idx, 10, :num_def, :num_off] = np.mod(frame_df.loc[def_ids, 'o'].values[:,None] - frame_df.loc[frame_df.position == 'QB', 'o'].values, 360)\n",
    "        if bad_play:\n",
    "            continue\n",
    "        data_dims[valid_plays] = last_frame - first_frame, num_def, num_off\n",
    "        data_extras[valid_plays] = play_df.los.iloc[0]\n",
    "        data_Y[valid_plays] = play_df.coverage_code.iloc[0]\n",
    "        valid_plays += 1\n",
    "    data_X = data_X[:valid_plays]\n",
    "    data_dims = data_dims[:valid_plays]\n",
    "    data_extras = data_extras[:valid_plays]\n",
    "    data_Y = data_Y[:valid_plays]\n",
    "    return data_X, data_dims, data_extras, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_save_path = join(out_dir, 'train_test_data.npz')\n",
    "if not os.path.isfile(data_save_path):\n",
    "    data_X, data_dims, data_extras, data_Y = gen_data()\n",
    "    np.savez(data_save_path, x=data_X, dims=data_dims, extras=data_extras, y=data_Y)\n",
    "else:\n",
    "    saved_data = np.load(data_save_path)\n",
    "    data_X, data_dims, data_extras, data_Y = saved_data['x'], saved_data['dims'], saved_data['extras'], saved_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_dims, test_dims, train_extras, test_extras, train_Y, test_Y = train_test_split(\n",
    "    data_X, data_dims, data_extras, data_Y, test_size=0.2\n",
    ")\n",
    "train_dataset = TensorDataset(*map(torch.tensor, [train_X, train_dims, train_extras, train_Y]))\n",
    "test_dataset = TensorDataset(*map(torch.tensor, [test_X, test_dims, test_extras, test_Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoverInner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 output_dim: int,\n",
    "                 conv_h: Union[int, Tuple[int, int]]=[128, 96],\n",
    "                 linear_h: Union[int, Tuple[int, int]]=[96, 256]):\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input features\n",
    "        :param output_dim: dimension of output embedding\n",
    "        :param conv_h: number of conv channels for each conv block.\n",
    "            int or tuple of 2 ints.\n",
    "        :param linear_h: number of hidden units for each linear layer.\n",
    "            int or tuple of 2 ints.    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if type(conv_h) is int:\n",
    "            conv_h = (conv_h, conv_h)\n",
    "        if type(linear_h) is int:\n",
    "            linear_h = (linear_h, linear_h)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_dim = output_dim\n",
    "        self.conv_h = conv_h\n",
    "        self.linear_h = linear_h\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(conv_h[0])\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_h[0], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "        )        \n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(conv_h[1], linear_h[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(linear_h[0]),\n",
    "            nn.Linear(*linear_h),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(linear_h[1]),\n",
    "            nn.Linear(linear_h[1], output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, dims):\n",
    "        # let (F', F\") and (..., F*) be conv_h and linear_h args to __init__\n",
    "        orig_shape = x.shape  # (B, T, F, D, O)\n",
    "        x = x.view(-1, *orig_shape[2:])  # (B*T, F, D, O)\n",
    "        \n",
    "        x = self.conv_block_1(x)  # (B*T, F', D, O)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F', D, O)\n",
    "        \n",
    "        # this block of code handles variable number of offensive players                \n",
    "        x_max = torch.stack([\n",
    "            F.max_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F', D)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv_block_2(x)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F\", D)\n",
    "        # this block of code handles variable number of defensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F\")\n",
    "        \n",
    "        x = self.linear_block(x)  # (B*T, F*)\n",
    "        \n",
    "        # restore shape\n",
    "        x = x.view(*orig_shape[:2], -1)  # (B, T, F*)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCoverOuter(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_layers: int=1,\n",
    "                 rnn_type: str='LSTM',\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param input_dim: input embedding dimension (per frame). same as Inner's output_dim\n",
    "        :param output_dim: output embedding dimension\n",
    "        :param hidden_size: dimension of LSTM hidden state\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param bidirectional: whether RNN is bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_dim, hidden_size, batch_first=True,\n",
    "                               num_layers=num_layers, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_dim, hidden_size, batch_first=True,\n",
    "                               num_layers=num_layers, bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(f'RNN type \"{rnn_type}\" not recognized.')\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 1 + int(bidirectional)\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_layers * self.num_directions * hidden_size, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # x is (B, T, F*)\n",
    "        batch_size = x.shape[0]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, dims[:,0].cpu(), batch_first=True, enforce_sorted=False)\n",
    "        x = self.rnn(x)[1][0]  # last hidden state\n",
    "        x = x.view(self.num_layers, self.num_directions, batch_size, -1)\n",
    "        x = x.permute(2, 0, 1, 3).reshape(batch_size, -1)  # (B, F^)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeepCover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inner_args: dict,\n",
    "                 outer_args: dict,\n",
    "                 num_classes: int,\n",
    "                 num_extras: int\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param inner_args, outer_args: arguments to inner and outer networks\n",
    "        :param num_classes: number of coverage classes to classify into\n",
    "        :param num_extras: number of dimensions in extra input to classifier head\n",
    "        \"\"\"\n",
    "        assert inner_args['output_dim'] == outer_args['input_dim']\n",
    "        \n",
    "        super().__init__()\n",
    "        self.outer = DeepCoverOuter(**outer_args)\n",
    "        self.inner = DeepCoverInner(**inner_args)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_extras = num_extras\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.outer.output_dim + num_extras, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dims, extras):\n",
    "        x = self.inner(x, dims)\n",
    "        x = self.outer(x, dims)\n",
    "        x = self.classifier(torch.cat((x, extras), dim=1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, dataloader, loss_fn, optimizer=None, train=True):\n",
    "    if train:\n",
    "        assert optimizer is not None, 'Optimizer must be specified in train mode.'\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    overall_loss = correct = 0\n",
    "    for xb, dimb, exb, yb in dataloader:\n",
    "        xb, dimb, exb, yb = xb.to(device), dimb.to(device), exb.to(device), yb.to(device)\n",
    "        y_pred = model(xb, dimb, exb)\n",
    "        loss = loss_fn(y_pred, yb.long())\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        overall_loss += loss.item() * xb.shape[0]\n",
    "        correct += (torch.max(y_pred, 1)[1] == yb).sum().item()\n",
    "    acc = correct / len(dataloader.dataset)\n",
    "    overall_loss /= len(dataloader.dataset)\n",
    "    return acc, overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_exp(params, exp_dir, log_to_console=True):\n",
    "    \"\"\" Pre-exp variant check \"\"\"\n",
    "    model_params, training_params = params['model_params'], params['training_params']\n",
    "    if model_params['num_layers'] == 1 and model_params['dropout'] != 0:\n",
    "        # dropout makes no difference if there's only 1 layer so don't waste runs\n",
    "        return\n",
    "    models_dir = join(exp_dir, 'models')\n",
    "    os.makedirs(models_dir)\n",
    "    \"\"\" Logging setup \"\"\"\n",
    "    stats = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    with open(join(exp_dir, 'variant.json'), 'w') as f:\n",
    "        jsonify(params, f)    \n",
    "    f_log = open(join(exp_dir, 'train.log'), 'w')\n",
    "    f_stats = open(join(exp_dir, 'stats.csv'), 'w')\n",
    "    writer = csv.DictWriter(f_stats, fieldnames=['train_loss', 'train_acc', 'test_loss', 'test_acc'])\n",
    "    writer.writeheader()\n",
    "    \n",
    "    \"\"\" Model setup \"\"\"\n",
    "    model = DeepCover(\n",
    "        inner_args={\n",
    "            'input_channels': model_params['num_features'],\n",
    "            'output_dim': model_params['embedding_dim']\n",
    "        },\n",
    "        outer_args={\n",
    "            'input_dim': model_params['embedding_dim'],\n",
    "            'hidden_size': model_params['hidden_size'],\n",
    "            'output_dim': model_params['outer_output_dim'],\n",
    "            'bidirectional': model_params['bidirectional'],\n",
    "            'rnn_type': model_params['rnn_type'],\n",
    "            'num_layers': model_params['num_layers']\n",
    "        },\n",
    "        num_classes=num_classes,\n",
    "        num_extras=num_extras\n",
    "    ).to(device)\n",
    "    \n",
    "    \"\"\" Training setup \"\"\"\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    num_epochs = training_params['num_epochs']\n",
    "    batch_size = training_params['batch_size']\n",
    "    save_freq = training_params['save_freq']\n",
    "    scheduler_type = training_params['scheduler_type']\n",
    "    best_test_acc = 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'])\n",
    "    if scheduler_type == 'exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=np.sqrt(0.1), patience=8, min_lr=9e-5)\n",
    "    else:\n",
    "        raise ValueError(f'Scheduler type \"{scheduler_type}\" not recognized.')\n",
    "\n",
    "    try:\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            \"\"\" Training \"\"\"\n",
    "            train_acc, train_loss = run_loop(model, train_loader, loss_fn, optimizer, train=True)\n",
    "\n",
    "            \"\"\" Evaluation\"\"\"\n",
    "            test_acc, test_loss = run_loop(model, test_loader, loss_fn, train=False)\n",
    "\n",
    "            scheduler.step(test_loss)\n",
    "            \"\"\" Logging \"\"\"\n",
    "            stats['train_loss'].append(train_loss)\n",
    "            stats['test_loss'].append(test_loss)\n",
    "            stats['train_acc'].append(train_acc)\n",
    "            stats['test_acc'].append(test_acc)\n",
    "            logger(f'Epoch {epoch+1:>2}/{num_epochs} | TrLoss {train_loss:>8.5f} | TrAcc {100*train_acc:>5.2f} | TeLoss {test_loss:>8.5f} | TeAcc {100*test_acc:>5.2f}',\n",
    "                  file=f_log, log_to_console=log_to_console)\n",
    "            writer.writerow({'train_loss': train_loss, 'train_acc': 100*train_acc, 'test_loss': test_loss, 'test_acc': 100*test_acc})\n",
    "            if (epoch >= save_freq and (best_test_acc := max(test_acc, best_test_acc)) == test_acc) or \\\n",
    "                (epoch+1) % save_freq == 0 or epoch == num_epochs - 1: \n",
    "                # new best after halfway, or save period, or last epoch\n",
    "                filename = datetime_str() + '.pt'\n",
    "                torch.save(model.state_dict(), join(models_dir, filename))\n",
    "                logger(f'Model saved at {join(exp_dir, filename)}', file=f_log, log_to_console=log_to_console)\n",
    "    except KeyboardInterrupt as e:\n",
    "        if input('Do you want to save the model? [y/N]: ').lower()[0] == 'y':\n",
    "            filename = datetime_str() + '.pt'\n",
    "            torch.save(model.state_dict(), join(models_dir, filename))\n",
    "            logger(f'Model saved at {join(exp_dir, filename)}', file=f_log, log_to_console=log_to_console)\n",
    "        raise e\n",
    "    finally:\n",
    "        f_log.close()\n",
    "        f_stats.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(model_space, training_space, verbose=True):\n",
    "    \"\"\"\n",
    "    :param model_space: dict mapping model hyperparams to list of values to search over\n",
    "    :param training_space: dict mapping training hyperparams to list of values to search over\n",
    "    Launches experiments sweeping over possible combinations of hyperparameters, saving resulting\n",
    "    experiment directories in the returned directory path.\n",
    "    \"\"\"\n",
    "    save_dir = join(exps_dir, datetime_str())\n",
    "    if verbose:\n",
    "        print(f'Running sweep in {save_dir}...')\n",
    "    model_combos = sweep(model_space)\n",
    "    training_combos = sweep(training_space)\n",
    "    num_exps = len(model_combos) * len(training_combos)\n",
    "    for exp_id, (model_params, training_params) in enumerate(itertools.product(model_combos, training_combos)):\n",
    "        if verbose:\n",
    "            print(f'Launching experiment {exp_id+1}/{num_exps}...')\n",
    "        params = {'model_params': model_params, 'training_params': training_params}\n",
    "        exp_dir = join(save_dir, str(exp_id).zfill(4))\n",
    "        run_exp(params, exp_dir, log_to_console=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run sweep over hyperparameters\n",
    "model_space = dict(\n",
    "    num_features=[num_features],\n",
    "    num_classes=[num_classes],\n",
    "    embedding_dim=[8, 32, 128],  # per frame\n",
    "    outer_output_dim=[8, 32],\n",
    "    hidden_size=[8, 32, 128],\n",
    "    bidirectional=[True],\n",
    "    rnn_type=['LSTM', 'GRU'],\n",
    "    num_layers=[1, 3],\n",
    "    dropout=[0]\n",
    ")\n",
    "training_space = dict(\n",
    "    num_epochs=[60],\n",
    "    batch_size=[64],\n",
    "    save_freq=[8],\n",
    "    learning_rate=[1e-3],\n",
    "    scheduler_type=['plateau']\n",
    ")\n",
    "run_sweep(model_space, training_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trained on week 1\n",
    "# best_model_path = join(models_dir, '03032021_040817/03032021_042056.pt')\n",
    "# best_model_path = join(models_dir, '03042021_070643/03042021_073225.pt')\n",
    "# best_model_path = join(models_dir, '03032021_092502/03032021_100205.pt')\n",
    "# # trained on weeks 1-4\n",
    "# best_model_path = join(models_dir, '03042021_135230/03042021_155248.pt')\n",
    "# best_model_path = join(models_dir, '03042021_173200/03042021_225324.pt')\n",
    "# best_model_path = join(models_dir, '03052021_053603/03052021_145102.pt')\n",
    "with open(join(os.path.dirname(best_model_path), 'variant.json'), 'r') as f:\n",
    "    variant = json.load(f)\n",
    "best_model = DeepCover(\n",
    "    inner_args={\n",
    "        'input_channels': num_features,\n",
    "        'output_dim': variant['embedding_dim']\n",
    "    },\n",
    "    outer_type=variant.get('outer_type', 'LSTM'),  # back compat.\n",
    "    outer_args={\n",
    "        'input_dim': variant['embedding_dim'],\n",
    "        'hidden_size': variant['hidden_size'],\n",
    "        'num_classes': num_classes,\n",
    "        'bidirectional': variant['bidirectional']\n",
    "    }\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "y_preds = []\n",
    "ys = []\n",
    "correct = 0\n",
    "dataloader = test_loader\n",
    "for xb, dimb, yb in dataloader:\n",
    "    xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "    y_logits = best_model(xb, dimb)\n",
    "    y_pred = torch.max(y_logits, 1)[1]\n",
    "    y_preds.append(y_pred)\n",
    "    ys.append(yb)\n",
    "    correct += (y_pred == yb).sum()\n",
    "acc = correct / len(dataloader.dataset)\n",
    "print(f'Accuracy: {acc*100:.3f}%')\n",
    "y_preds = torch.cat(y_preds).flatten()\n",
    "ys = torch.cat(ys).flatten()\n",
    "cm = confusion_matrix(ys, y_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
