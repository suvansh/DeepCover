{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using personal config for user: suvansh\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from utils import get_repo_dir, datetime_str, logger, jsonify\n",
    "from config.config import *\n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Add yard line feature\n",
    "* Add GPU support\n",
    "* Add hyperparameter sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial variables and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = 30  # truncate if snap-to-throw is > this. units: ds\n",
    "num_features = 10  # number of channels in input (vx, vy, etc)\n",
    "out_dir = join(get_repo_dir(), 'output')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "models_dir = join(out_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# week1_tracking = pd.read_csv(join(data_dir, 'week1_norm.csv'))\n",
    "# week1_coverage = pd.read_csv(join(data_dir, 'coverages_week1.csv'), dtype={'coverage': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge data\n",
    "# week1_data = pd.merge(week1_tracking, week1_coverage, how='right', on=['gameId', 'playId'])\n",
    "# week1_data['coverage_code'] = week1_data.coverage.cat.codes\n",
    "# num_classes = len(week1_data.coverage.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [01:04<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "weeks = range(1, 18)\n",
    "tracking_data = pd.concat([pd.read_csv(join(data_dir, f'week{week_num}_norm.csv')) for week_num in tqdm(weeks)], axis=0)\n",
    "coverage_data = pd.read_csv(join(data_dir, 'coverages_2018.csv')).dropna(subset=['coverage'])\n",
    "coverage_data.coverage.replace({'3 Seam': 'Cover 3 Zone', 'Cover 1 Double': 'Cover 1 Man'}, inplace=True)\n",
    "coverage_data = coverage_data[~coverage_data.coverage.isin(['Bracket', 'Mis'])]\n",
    "coverage_data.coverage = coverage_data.coverage.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Cover 0 Man',\n",
       " 1: 'Cover 1 Man',\n",
       " 2: 'Cover 2 Man',\n",
       " 3: 'Cover 2 Zone',\n",
       " 4: 'Cover 3 Zone',\n",
       " 5: 'Cover 4 Zone',\n",
       " 6: 'Cover 6 Zone',\n",
       " 7: 'Goal Line',\n",
       " 8: 'Prevent',\n",
       " 9: 'Red Zone'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data\n",
    "full_data = pd.merge(tracking_data, coverage_data, how='inner', left_on=['gameId', 'playId'], right_on=['game_id', 'play_id'])\n",
    "full_data['coverage_code'] = full_data.coverage.cat.codes\n",
    "coverage_label_map = dict(enumerate(full_data['coverage'].cat.categories))\n",
    "num_classes = len(coverage_label_map)\n",
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tracking_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO data aug. this is very slightly complicated by the fact that we have precomputed the vx, vy, ax, ay. will have to ask udit about how this is computed since it seems to be inconsistent with v_theta and v_mag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we don't have orientation relative to qb, so I don't think it makes sense to highlight the qb in the way the bdb winner does for rusher, since it would just be distance from qb and qb-relative speed/accel data right now, which I doubt is _extra_ useful for predicting coverage (?). so for now, we'll do 11x11 of off x def with features:\n",
    "* relative: x, y, vx, vy, ax, ay\n",
    "* absolute: vx, vy, ax, ay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    grouped = full_data.groupby(['gameId', 'playId'])\n",
    "    # TODO can change 11s to max number of off and def players in data\n",
    "    data_X = np.empty((len(grouped), max_num_frames, num_features, 11, 11), dtype=np.float32)  # (P, T, F, D, O): play, frame, feature, def, off\n",
    "    data_dims = np.empty((len(grouped), 3), dtype=np.int32) # (P, 3) contains (t, d, o): num frames, num def, num off on each play\n",
    "    data_Y = np.empty(len(grouped), dtype=np.int32)  # (P,)\n",
    "\n",
    "    valid_plays = 0\n",
    "    for (game_id, play_id), play_df in tqdm(grouped):\n",
    "        try:\n",
    "            first_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'ball_snap')].frameId.iloc[0]\n",
    "            play_end_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event.isin(['pass_forward', 'pass_shovel', 'qb_sack', 'qb_strip_sack', 'tackle']))].frameId.iloc[0]\n",
    "        except:\n",
    "            print(f'({game_id}, {play_id}) failed. events were {play_df.event.unique()}')\n",
    "            continue\n",
    "        last_frame = min(first_frame + max_num_frames - 1, play_end_frame)\n",
    "        play_df = play_df.loc[play_df.frameId.between(first_frame, last_frame)]\n",
    "\n",
    "        num_def, num_off = 0, 0\n",
    "        for frame_idx, (frame_id, frame_df) in enumerate(play_df.groupby('frameId')):\n",
    "            def_ids = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "            num_def = len(def_ids)\n",
    "            off_ids = frame_df[frame_df.team_pos == 'OFF'].index\n",
    "            num_off = len(off_ids)\n",
    "            if num_def > 11 or num_off > 11:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                break\n",
    "            if num_def < 3 or num_off < 3:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                break\n",
    "\n",
    "            outer_sub = np.subtract.outer(\n",
    "                frame_df.loc[off_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values,\n",
    "                frame_df.loc[def_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values\n",
    "            )\n",
    "            if np.isnan(outer_sub).any():\n",
    "                print(f'({game_id}, {play_id}), frame {frame_id} has NaNs')\n",
    "                break\n",
    "            # einsum explanation: the two i's get rid of subtraction across cols\n",
    "            # k before j reorders def before off since output dims in alph. order\n",
    "            data_X[valid_plays, frame_idx, :6, :num_def, :num_off] = np.einsum('kiji->ijk', outer_sub)\n",
    "            data_X[valid_plays, frame_idx, -4:, :num_def, :num_off] = frame_df.loc[def_ids, ['v_x', 'v_y', 'a_x', 'a_y']].values.T[...,None]\n",
    "        if num_def > 11 or num_off > 11 or num_def < 3 or num_off < 3:\n",
    "            continue\n",
    "        data_Y[valid_plays] = play_df.coverage_code.iloc[0]\n",
    "        data_dims[valid_plays] = last_frame - first_frame, num_def, num_off\n",
    "        valid_plays += 1\n",
    "    data_X = data_X[:valid_plays]\n",
    "    data_dims = data_dims[:valid_plays]\n",
    "    data_Y = data_Y[:valid_plays]\n",
    "    return data_X, data_dims, data_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1573/18957 [03:56<31:31,  9.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018091605, 2715), num_def 32, num_off 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2309/18957 [05:09<24:19, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018092000, 1539), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2457/18957 [05:24<18:13, 15.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018092301, 453), frame 14 has NaNs\n",
      "(2018092301, 520), frame 17 has NaNs\n",
      "(2018092301, 565), frame 11 has NaNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2463/18957 [05:24<32:52,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018092301, 949), frame 35 has NaNs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2769/18957 [05:54<22:57, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018092305, 1553), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3443/18957 [06:59<23:46, 10.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018092400, 1867), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4014/18957 [07:56<30:28,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018093005, 168), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 5888/18957 [11:02<18:06, 12.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018100800 951 failed. events were: ['None' 'ball_snap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 6386/18957 [12:00<23:36,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018101404 2003 failed. events were: ['None' 'ball_snap' 'pass_forward' 'pass_arrived'\n",
      " 'pass_outcome_incomplete']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 6952/18957 [13:02<15:57, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018101412, 1559), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 6958/18957 [13:02<16:47, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018101412, 1922), num_def 8, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 6962/18957 [13:03<17:32, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018101412 2241 failed. events were: ['None' 'huddle_break_offense' 'line_set' 'shift' 'ball_snap'\n",
      " 'pass_forward' 'pass_arrived' 'pass_outcome_caught' 'first_contact'\n",
      " 'tackle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 7247/18957 [13:31<16:12, 12.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102101, 2734), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7397/18957 [13:48<18:11, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018102103 2287 failed. events were: ['None' 'ball_snap' 'first_contact' 'out_of_bounds']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 7562/18957 [14:04<16:12, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018102105 3355 failed. events were: ['None' 'ball_snap' 'play_action' 'run' 'out_of_bounds']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 8133/18957 [15:00<14:45, 12.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102500, 1655), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 8754/18957 [16:04<14:55, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018102807 3092 failed. events were: ['None' 'ball_snap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8841/18957 [16:13<15:26, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102809, 298), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8890/18957 [16:18<17:13,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102809, 3344), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8919/18957 [16:21<16:07, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102810, 436), num_def 9, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9004/18957 [16:30<14:58, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018102811, 1119), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 9415/18957 [17:11<16:56,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018110402, 2502), num_def 7, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 9615/18957 [17:37<15:10, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018110405, 1062), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 9691/18957 [17:45<17:36,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018110406, 1226), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 11414/18957 [20:50<12:17, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018111803, 178), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 11999/18957 [22:00<17:26,  6.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018111900 1696 failed. events were: ['None' 'ball_snap' 'run' 'out_of_bounds']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 12321/18957 [22:34<10:41, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018112500, 765), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 12344/18957 [22:36<08:32, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018112500, 1824), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12617/18957 [23:04<09:11, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018112504, 3114), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 13027/18957 [23:47<09:02, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018112510, 937), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13557/18957 [24:36<07:35, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018120204, 3828), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 14364/18957 [25:57<07:22, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018120600, 1343), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 14531/18957 [26:12<05:38, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018120901 2552 failed. events were: ['None' 'ball_snap' 'first_contact' 'safety']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14834/18957 [26:41<05:19, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018120905, 1426), num_def 36, num_off 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 15319/18957 [27:27<05:15, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018120911, 4000), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15650/18957 [28:03<04:38, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018121500, 1129), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15696/18957 [28:08<05:08, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018121500, 3860), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 16138/18957 [28:51<03:58, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018121604 4235 failed. events were: ['None' 'ball_snap' 'first_contact']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16631/18957 [29:42<04:19,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018121700, 1865), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16636/18957 [29:43<03:35, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018121700, 2267), num_def 10, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 17226/18957 [30:39<03:07,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018122307, 4434), num_def 0, num_off 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 17941/18957 [32:08<01:11, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018123001, 435), num_def 34, num_off 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18128/18957 [32:27<01:17, 10.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018123003 3357 failed. events were: ['None' 'ball_snap' 'first_contact']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 18185/18957 [32:32<01:02, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018123004, 2309), num_def 8, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 18502/18957 [33:04<00:39, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018123009 2207 failed. events were: ['None' 'ball_snap' 'first_contact']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 18543/18957 [33:08<00:35, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2018123010, 334), num_def 11, num_off 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 18682/18957 [33:21<00:33,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018123012 60 failed. events were: ['None' 'ball_snap']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18957/18957 [33:52<00:00,  9.33it/s]\n"
     ]
    }
   ],
   "source": [
    "data_save_path = join(out_dir, 'full_data.npz')\n",
    "# NOTE: uncomment to generate data and save\n",
    "data_X, data_dims, data_Y = gen_data()\n",
    "np.savez(data_save_path, x=data_X, dims=data_dims, y=data_Y)\n",
    "# NOTE: uncomment to load from save\n",
    "# saved_data = np.load(data_save_path)\n",
    "# data_X, data_dims, data_Y = saved_data['x'], saved_data['dims'], saved_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_dims, test_dims, train_Y, test_Y = train_test_split(\n",
    "    data_X, data_dims, data_Y, test_size=0.2, random_state=12\n",
    ")\n",
    "train_dataset = TensorDataset(*map(torch.tensor, [train_X, train_dims, train_Y]))\n",
    "test_dataset = TensorDataset(*map(torch.tensor, [test_X, test_dims, test_Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoverInner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 output_dim: int,\n",
    "                 conv_h: Union[int, Tuple[int, int]]=[128, 96],\n",
    "                 linear_h: Union[int, Tuple[int, int]]=[96, 256]):\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input features\n",
    "        :param output_dim: dimension of output embedding\n",
    "        :param conv_h: number of conv channels for each conv block.\n",
    "            int or tuple of 2 ints.\n",
    "        :param linear_h: number of hidden units for each linear layer.\n",
    "            int or tuple of 2 ints.    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if type(conv_h) is int:\n",
    "            conv_h = (conv_h, conv_h)\n",
    "        if type(linear_h) is int:\n",
    "            linear_h = (linear_h, linear_h)\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(conv_h[0])\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_h[0], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "        )\n",
    "        self.conv_h = conv_h\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(conv_h[1], linear_h[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(linear_h[0]),\n",
    "            nn.Linear(*linear_h),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(linear_h[1]),\n",
    "            nn.Linear(linear_h[1], output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, dims):\n",
    "        # let (F', F\") and (..., F*) be conv_h and linear_h args to __init__\n",
    "        orig_shape = x.shape  # (B, T, F, D, O)\n",
    "        x = x.view(-1, *orig_shape[2:])  # (B*T, F, D, O)\n",
    "        \n",
    "        x = self.conv_block_1(x)  # (B*T, F', D, O)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F', D, O)\n",
    "        \n",
    "        # this block of code handles variable number of offensive players                \n",
    "        x_max = torch.stack([\n",
    "            F.max_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F', D)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv_block_2(x)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F\", D)\n",
    "        # this block of code handles variable number of defensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F\")\n",
    "        \n",
    "        x = self.linear_block(x)  # (B*T, F*)\n",
    "        \n",
    "        # restore shape\n",
    "        x = x.view(*orig_shape[:2], -1)  # (B, T, F*)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCoverOuterLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_classes: int,\n",
    "                 num_layers: int=1,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param input_dim: input embedding dimension (per frame). same as Inner's output_dim\n",
    "        :param hidden_size: dimension of LSTM hidden state\n",
    "        :param num_classes: number of coverage classes\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param bidirectional: whether RNN is bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_size, batch_first=True,\n",
    "                           num_layers=num_layers, bidirectional=bidirectional)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 1 + int(bidirectional)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_layers * self.num_directions * hidden_size, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # x is (B, T, F*)\n",
    "        batch_size = x.shape[0]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, dims[:,0], batch_first=True, enforce_sorted=False)\n",
    "        x = self.rnn(x)[1][0]  # last hidden state\n",
    "        x = x.view(self.num_layers, self.num_directions, batch_size, -1)\n",
    "        x = x.permute(2, 0, 1, 3).reshape(batch_size, -1)  # (B, F^)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeepCover(nn.Module):\n",
    "    type_class_map = {\n",
    "        'LSTM': DeepCoverOuterLSTM\n",
    "    }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 inner_args: dict,\n",
    "                 outer_args: dict,\n",
    "                 outer_type: str='rnn'\n",
    "                ):\n",
    "        assert inner_args['output_dim'] == outer_args['input_dim']\n",
    "        assert (DeepCoverOuter := self.type_class_map.get(outer_type))\n",
    "        \n",
    "        super().__init__()\n",
    "        self.outer = DeepCoverOuter(**outer_args)\n",
    "        self.inner = DeepCoverInner(**inner_args)\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        x = self.inner(x, dims)\n",
    "        x = self.outer(x, dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters (TODO sweep?)\n",
    "model_params = dict(\n",
    "    embedding_dim=32,\n",
    "    hidden_size=32,\n",
    "    bidirectional=True,\n",
    "    outer_type='LSTM'\n",
    ")\n",
    "model = DeepCover(\n",
    "    inner_args={\n",
    "        'input_channels': num_features,\n",
    "        'output_dim': model_params['embedding_dim']\n",
    "    },\n",
    "    outer_type=model_params['outer_type'],\n",
    "    outer_args={\n",
    "        'input_dim': model_params['embedding_dim'],\n",
    "        'hidden_size': model_params['hidden_size'],\n",
    "        'num_classes': num_classes,\n",
    "        'bidirectional': model_params['bidirectional']\n",
    "    }\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "exp_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "dynamic_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=np.sqrt(0.1), patience=8, min_lr=9e-5, verbose=True)\n",
    "scheduler = dynamic_scheduler\n",
    "\n",
    "# adding optimizer and scheduler to this so that they get logged\n",
    "training_params = dict(\n",
    "    num_epochs=80,\n",
    "    batch_size=64,\n",
    "    loss_fn=nn.NLLLoss(),\n",
    "    save_freq=8,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'], shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=training_params['batch_size'], shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, dataloader, loss_fn, optimizer=None, train=True):\n",
    "    if train:\n",
    "        assert optimizer is not None, 'Optimizer must be specified in train mode.'\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss = correct = 0\n",
    "    for xb, dimb, yb in dataloader:\n",
    "        xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "        y_pred = model(xb, dimb)\n",
    "        loss = loss_fn(y_pred, yb.long())\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss += loss.item() * xb.shape[0]\n",
    "        correct += (torch.max(y_pred, 1)[1] == yb).sum().item()\n",
    "    acc = correct / len(dataloader.dataset)\n",
    "    loss /= len(dataloader.dataset)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/80 [18:03<23:47:01, 1083.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/80 | TrLoss  0.00117 | TrAcc 71.23 | TeLoss  0.00476 | TeAcc 69.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▎         | 2/80 [29:49<21:01:22, 970.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/80 | TrLoss  0.00181 | TrAcc 72.64 | TeLoss  0.00126 | TeAcc 71.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 3/80 [41:13<18:54:56, 884.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/80 | TrLoss  0.00139 | TrAcc 73.08 | TeLoss  0.00123 | TeAcc 73.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 4/80 [52:23<17:18:47, 820.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/80 | TrLoss  0.00097 | TrAcc 73.41 | TeLoss  0.00406 | TeAcc 72.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▋         | 5/80 [1:03:48<16:14:29, 779.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/80 | TrLoss  0.00120 | TrAcc 74.50 | TeLoss  0.00098 | TeAcc 73.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 6/80 [1:15:18<15:28:18, 752.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/80 | TrLoss  0.00148 | TrAcc 75.02 | TeLoss  0.00144 | TeAcc 75.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 7/80 [1:27:19<15:04:15, 743.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/80 | TrLoss  0.00113 | TrAcc 75.91 | TeLoss  0.00050 | TeAcc 75.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 8/80 [1:37:29<14:03:51, 703.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/80 | TrLoss  0.00076 | TrAcc 75.66 | TeLoss  0.00093 | TeAcc 75.63\n",
      "Model saved at /Users/sanjeev/Documents/Personal/Football/DeepCover/output/models/03052021_053603/03052021_071333.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█▏        | 9/80 [1:47:33<13:17:03, 673.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/80 | TrLoss  0.00070 | TrAcc 76.63 | TeLoss  0.00055 | TeAcc 73.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 10/80 [1:57:40<12:42:28, 653.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/80 | TrLoss  0.00088 | TrAcc 77.51 | TeLoss  0.00092 | TeAcc 70.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 11/80 [2:07:49<12:16:23, 640.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/80 | TrLoss  0.00053 | TrAcc 77.24 | TeLoss  0.00210 | TeAcc 76.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 12/80 [2:17:57<11:54:41, 630.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/80 | TrLoss  0.00098 | TrAcc 78.04 | TeLoss  0.00216 | TeAcc 76.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▋        | 13/80 [2:28:19<11:41:16, 628.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/80 | TrLoss  0.00066 | TrAcc 78.56 | TeLoss  0.00341 | TeAcc 77.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 14/80 [2:38:28<11:24:27, 622.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/80 | TrLoss  0.00115 | TrAcc 78.25 | TeLoss  0.00157 | TeAcc 76.16\n"
     ]
    }
   ],
   "source": [
    "stats = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "exp_dir = join(models_dir, datetime_str())\n",
    "os.makedirs(exp_dir)\n",
    "with open(join(exp_dir, 'variant.json'), 'w') as f:\n",
    "    jsonify({**model_params, **training_params}, f)\n",
    "log_to_file, f = True, None\n",
    "num_epochs = training_params['num_epochs']\n",
    "batch_size = training_params['batch_size']\n",
    "loss_fn = training_params['loss_fn']\n",
    "save_freq = training_params['save_freq']\n",
    "\n",
    "try:\n",
    "    best_test_acc = 0\n",
    "    if log_to_file:\n",
    "        f = open(join(exp_dir, 'train.log'), 'w')\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \"\"\" Training \"\"\"\n",
    "        train_acc, train_loss = run_loop(model, train_loader, loss_fn, optimizer, train=True)\n",
    "        \n",
    "        \"\"\" Evaluation\"\"\"\n",
    "        test_acc, test_loss = run_loop(model, test_loader, loss_fn, train=False)\n",
    "        \n",
    "        scheduler.step(test_loss)\n",
    "        \"\"\" Logging \"\"\"\n",
    "        stats['train_loss'].append(train_loss)\n",
    "        stats['test_loss'].append(test_loss)\n",
    "        stats['train_acc'].append(train_acc)\n",
    "        stats['test_acc'].append(test_acc)\n",
    "        logger(f'Epoch {epoch+1:>2}/{num_epochs} | TrLoss {train_loss:>8.5f} | TrAcc {100*train_acc:>5.2f} | TeLoss {test_loss:>8.5f} | TeAcc {100*test_acc:>5.2f}',\n",
    "              file=f, log_to_file=log_to_file)\n",
    "        if (epoch >= num_epochs // 4 and (best_test_acc := max(test_acc, best_test_acc)) == test_acc) or \\\n",
    "            (epoch+1) % save_freq == 0 or epoch == num_epochs - 1: \n",
    "            # new best after halfway, or save period, or last epoch\n",
    "            filename = datetime_str() + '.pt'\n",
    "            torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "            logger(f'Model saved at {join(exp_dir, filename)}', file=f, log_to_file=log_to_file)\n",
    "    if log_to_file:\n",
    "        f.close()\n",
    "except KeyboardInterrupt as e:\n",
    "    if input('Do you want to save the model? [y/N]: ').lower()[0] == 'y':\n",
    "        filename = datetime_str() + '.pt'\n",
    "        torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "        logger(f'Model saved at {join(exp_dir, filename)}', file=f, log_to_file=log_to_file)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained on week 1\n",
    "best_model_path = join(models_dir, '03032021_040817/03032021_042056.pt')\n",
    "best_model_path = join(models_dir, '03042021_070643/03042021_073225.pt')\n",
    "best_model_path = join(models_dir, '03032021_092502/03032021_100205.pt')\n",
    "# trained on weeks 1-4\n",
    "best_model_path = join(models_dir, '03042021_135230/03042021_155248.pt')\n",
    "best_model_path = join(models_dir, '03042021_173200/03042021_225324.pt')\n",
    "with open(join(os.path.dirname(best_model_path), 'variant.json'), 'r') as f:\n",
    "    variant = json.load(f)\n",
    "best_model = DeepCover(\n",
    "    inner_args={\n",
    "        'input_channels': num_features,\n",
    "        'output_dim': variant['embedding_dim']\n",
    "    },\n",
    "    outer_type=variant.get('outer_type', 'LSTM'),  # back compat.\n",
    "    outer_args={\n",
    "        'input_dim': variant['embedding_dim'],\n",
    "        'hidden_size': variant['hidden_size'],\n",
    "        'num_classes': num_classes,\n",
    "        'bidirectional': variant['bidirectional']\n",
    "    }\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "y_preds = []\n",
    "ys = []\n",
    "correct = 0\n",
    "dataloader = test_loader\n",
    "for xb, dimb, yb in dataloader:\n",
    "    xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "    y_logits = best_model(xb, dimb)\n",
    "    y_pred = torch.max(y_logits, 1)[1]\n",
    "    y_preds.append(y_pred)\n",
    "    ys.append(yb)\n",
    "    correct += (y_pred == yb).sum()\n",
    "acc = correct / len(dataloader.dataset)\n",
    "print(f'Accuracy: {acc*100:.3f}%')\n",
    "y_preds = torch.cat(y_preds).flatten()\n",
    "ys = torch.cat(ys).flatten()\n",
    "cm = confusion_matrix(ys, y_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
