{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using personal config for user: suvansh\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import join\n",
    "from utils import get_repo_dir, datetime_str\n",
    "from config.config import *\n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial variables and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = 30  # truncate if snap-to-throw is > this. units: ds\n",
    "out_dir = join(get_repo_dir(), 'output')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "models_dir = join(out_dir, 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "week1_tracking = pd.read_csv(join(data_dir, 'week1_norm.csv'))\n",
    "week1_coverage = pd.read_csv(join(data_dir, 'coverages_week1.csv'), dtype={'coverage': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "week1_data = pd.merge(week1_tracking, week1_coverage, how='right', on=['gameId', 'playId'])\n",
    "week1_data['coverage_code'] = week1_data.coverage.cat.codes\n",
    "num_classes = len(week1_data.coverage.cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO data aug. this is very slightly complicated by the fact that we have precomputed the vx, vy, ax, ay. will have to ask udit about how this is computed since it seems to be inconsistent with v_theta and v_mag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we don't have orientation relative to qb, so I don't think it makes sense to highlight the qb in the way the bdb winner does for rusher, since it would just be distance from qb and qb-relative speed/accel data right now, which I doubt is _extra_ useful for predicting coverage (?). so for now, we'll do 11x11 of off x def with features:\n",
    "* relative: x, y, vx, vy, ax, ay\n",
    "* absolute: vx, vy, ax, ay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [01:57<00:00,  8.78it/s]\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "\n",
    "grouped = week1_data.groupby(['gameId', 'playId'])\n",
    "# TODO can change 11s to max number of off and def players in data\n",
    "data_X = np.empty((len(grouped), max_num_frames, num_features, 11, 11), dtype=np.float32)  # (P, T, F, D, O): play, frame, feature, def, off\n",
    "data_dims = np.empty((len(grouped), 3), dtype=np.int32) # (P, 3) contains (t, d, o): num frames, num def, num off on each play\n",
    "data_Y = np.empty(len(grouped), dtype=np.int32)  # (P,)\n",
    "\n",
    "valid_plays = 0\n",
    "for game_play, play_df in tqdm(grouped):\n",
    "    if 'pass_forward' not in play_df.event.unique():\n",
    "        continue\n",
    "    first_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'ball_snap')].frameId.iloc[0]\n",
    "    play_end_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'pass_forward')].frameId.iloc[0]\n",
    "    last_frame = min(first_frame + max_num_frames - 1, play_end_frame)\n",
    "    play_df = play_df.loc[play_df.frameId.between(first_frame, last_frame)]\n",
    "    \n",
    "    num_def, num_off = 0, 0\n",
    "    for frame_idx, (frame_id, frame_df) in enumerate(play_df.groupby('frameId')):\n",
    "        def_ids = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "        num_def = len(def_ids)\n",
    "        off_ids = frame_df[frame_df.team_pos == 'OFF'].index\n",
    "        num_off = len(off_ids)\n",
    "    \n",
    "        outer_sub = np.subtract.outer(\n",
    "            frame_df.loc[off_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values,\n",
    "            frame_df.loc[def_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values\n",
    "        )\n",
    "        # einsum explanation: the two i's get rid of subtraction across cols\n",
    "        # k before j reorders def before off since output dims in alph. order\n",
    "        data_X[valid_plays, frame_idx, :6, :num_def, :num_off] = np.einsum('kiji->ijk', outer_sub)\n",
    "        data_X[valid_plays, frame_idx, -4:, :num_def, :num_off] = frame_df.loc[def_ids, ['v_x', 'v_y', 'a_x', 'a_y']].values.T[...,None]\n",
    "    data_Y[valid_plays] = play_df.coverage_code.iloc[0]\n",
    "    data_dims[valid_plays] = last_frame - first_frame, num_def, num_off\n",
    "    valid_plays += 1\n",
    "data_X = data_X[:valid_plays]\n",
    "data_dims = data_dims[:valid_plays]\n",
    "data_Y = data_Y[:valid_plays]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_save_path = join(out_dir, 'week1_data.npz')\n",
    "# NOTE: uncomment to save\n",
    "np.savez(data_save_path, x=data_X, dims=data_dims, y=data_Y)\n",
    "# NOTE: uncomment to load from save\n",
    "# saved_data = np.load(data_save_path)\n",
    "# data_X, data_dims, data_Y = saved_data['x'], saved_data['dims'], saved_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_dims, test_dims, train_Y, test_Y = train_test_split(\n",
    "    data_X, data_dims, data_Y, test_size=0.2, random_state=12\n",
    ")\n",
    "train_dataset = TensorDataset(*map(torch.tensor, [train_X, train_dims, train_Y]))\n",
    "test_dataset = TensorDataset(*map(torch.tensor, [test_X, test_dims, test_Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoverInner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 output_dim: int,\n",
    "                 conv_h: Union[int, Tuple[int, int]]=[128, 96],\n",
    "                 linear_h: Union[int, Tuple[int, int]]=[96, 256]):\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input features\n",
    "        :param output_dim: dimension of output embedding\n",
    "        :param conv_h: number of conv channels for each conv block.\n",
    "            int or tuple of 2 ints.\n",
    "        :param linear_h: number of hidden units for each linear layer.\n",
    "            int or tuple of 2 ints.    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if type(conv_h) is int:\n",
    "            conv_h = (conv_h, conv_h)\n",
    "        if type(linear_h) is int:\n",
    "            linear_h = (linear_h, linear_h)\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(conv_h[0])\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_h[0], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "        )\n",
    "        self.conv_h = conv_h\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(conv_h[1], linear_h[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(linear_h[0]),\n",
    "            nn.Linear(*linear_h),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(linear_h[1]),\n",
    "            nn.Linear(linear_h[1], output_dim)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, dims):\n",
    "        # let (F', F\") and (..., F*) be conv_h and linear_h args to __init__\n",
    "        orig_shape = x.shape  # (B, T, F, D, O)\n",
    "        x = x.view(-1, *orig_shape[2:])  # (B*T, F, D, O)\n",
    "        \n",
    "        x = self.conv_block_1(x)  # (B*T, F', D, O)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F', D, O)\n",
    "        # this block of code handles variable number of offensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F', D)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.conv_block_2(x)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F\", D)\n",
    "        # this block of code handles variable number of defensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F\")\n",
    "        \n",
    "        x = self.linear_block(x)  # (B*T, F*)\n",
    "        \n",
    "        # restore shape\n",
    "        x = x.view(*orig_shape[:2], -1)  # (B, T, F*)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCoverOuterLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_classes: int,\n",
    "                 num_layers: int=1,\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param input_dim: input embedding dimension (per frame). same as Inner's output_dim\n",
    "        :param hidden_size: dimension of LSTM hidden state\n",
    "        :param num_classes: number of coverage classes\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param bidirectional: whether RNN is bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_size, batch_first=True,\n",
    "                           num_layers=num_layers, bidirectional=bidirectional)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 1 + int(bidirectional)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_layers * self.num_directions * hidden_size, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # x is (B, T, F*)\n",
    "        batch_size = x.shape[0]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, dims[:,0], batch_first=True, enforce_sorted=False)\n",
    "        x = self.rnn(x)[1][0]  # last hidden state\n",
    "        x = x.view(self.num_layers, self.num_directions, batch_size, -1)\n",
    "        x = x.permute(2, 0, 1, 3).reshape(batch_size, -1)  # (B, F^)\n",
    "        # TODO may have to contiguous() this.\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeepCover(nn.Module):\n",
    "    type_class_map = {\n",
    "        'LSTM': DeepCoverOuterLSTM\n",
    "    }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 inner_args: dict,\n",
    "                 outer_args: dict,\n",
    "                 outer_type: str='rnn'\n",
    "                ):\n",
    "        assert inner_args['output_dim'] == outer_args['input_dim']\n",
    "        assert (DeepCoverOuter := self.type_class_map.get(outer_type))\n",
    "        \n",
    "        super().__init__()\n",
    "        self.outer = DeepCoverOuter(**outer_args)\n",
    "        self.inner = DeepCoverInner(**inner_args)\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        x = self.inner(x, dims)\n",
    "        x = self.outer(x, dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters (TODO sweep?)\n",
    "embedding_dim = 96\n",
    "hidden_size = 64\n",
    "bidirectional = True\n",
    "\n",
    "model = DeepCover(\n",
    "    inner_args={\n",
    "        'input_channels': num_features,\n",
    "        'output_dim': embedding_dim\n",
    "    },\n",
    "    outer_type='LSTM',\n",
    "    outer_args={\n",
    "        'input_dim': embedding_dim,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_classes': num_classes,\n",
    "        'bidirectional': bidirectional\n",
    "    }\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "loss_fn = nn.NLLLoss()\n",
    "save_freq = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:42<06:19, 42.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/10 | TrLoss  1.84209 | TrAcc  0.38 | TeLoss  2.06525 | TeAcc  0.18\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055537.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [01:23<05:34, 41.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/10 | TrLoss  1.54910 | TrAcc  0.50 | TeLoss  1.91125 | TeAcc  0.28\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055618.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [02:04<04:52, 41.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/10 | TrLoss  1.31429 | TrAcc  0.55 | TeLoss  1.72785 | TeAcc  0.29\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055700.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [02:46<04:09, 41.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4/10 | TrLoss  1.09852 | TrAcc  0.63 | TeLoss  1.40184 | TeAcc  0.48\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055741.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [03:29<03:29, 42.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5/10 | TrLoss  0.90931 | TrAcc  0.72 | TeLoss  1.17864 | TeAcc  0.59\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055824.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [04:11<02:48, 42.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6/10 | TrLoss  0.76259 | TrAcc  0.77 | TeLoss  1.14370 | TeAcc  0.58\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055907.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [05:04<02:16, 45.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7/10 | TrLoss  0.64297 | TrAcc  0.80 | TeLoss  1.11585 | TeAcc  0.58\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_055959.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [05:56<01:34, 47.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8/10 | TrLoss  0.54006 | TrAcc  0.84 | TeLoss  0.98854 | TeAcc  0.64\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_060051.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [06:50<00:49, 49.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9/10 | TrLoss  0.47338 | TrAcc  0.86 | TeLoss  1.06574 | TeAcc  0.59\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_060145.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:45<00:00, 46.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | TrLoss  0.42482 | TrAcc  0.87 | TeLoss  1.13720 | TeAcc  0.58\n",
      "Model saved @ /Users/sanjeev/GoogleDrive/Personal/Football/DeepCover/output/models/03022021_055455/03022021_060241.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "exp_dir = join(models_dir, datetime_str())\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "try:\n",
    "    best_test_acc = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \"\"\" training \"\"\"\n",
    "        model.train()\n",
    "        train_loss = train_correct = 0\n",
    "        for xb, dimb, yb in train_loader:\n",
    "            xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "            y_pred = model(xb, dimb)\n",
    "            loss = loss_fn(y_pred, yb.long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * xb.shape[0]\n",
    "            train_correct += (torch.max(y_pred, 1)[1] == yb).sum().item()\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "        train_loss /= len(train_dataset)\n",
    "\n",
    "        \"\"\" evaluation \"\"\"\n",
    "        model.eval()\n",
    "        test_loss = test_correct = 0\n",
    "        for xb, dimb, yb in test_loader:\n",
    "            xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "            y_pred = model(xb, dimb)\n",
    "            loss = loss_fn(y_pred, yb.long())\n",
    "\n",
    "            test_loss += loss.item() * xb.shape[0]\n",
    "            test_correct += (torch.max(y_pred, 1)[1] == yb).sum().item()\n",
    "        test_acc = test_correct / len(test_dataset)\n",
    "        test_loss /= len(test_dataset)\n",
    "\n",
    "        stats['train_loss'].append(train_loss)\n",
    "        stats['test_loss'].append(test_loss)\n",
    "        stats['train_acc'].append(train_acc)\n",
    "        stats['test_acc'].append(test_acc)\n",
    "        print(f'Epoch {epoch+1:>2}/{num_epochs} | TrLoss {train_loss:>8.5f} | TrAcc {train_acc:>5.2f} | TeLoss {test_loss:>8.5f} | TeAcc {test_acc:>5.2f}')\n",
    "        if (best_test_acc := max(test_acc, best_test_acc)) == test_acc or \\\n",
    "            (epoch+1) % save_freq == 0 or epoch == num_epochs - 1:  # new best, or save period, or last epoch\n",
    "            filename = datetime_str() + '.pt'\n",
    "            torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "            print(f'Model saved @ {join(exp_dir, filename)}')\n",
    "            \n",
    "except KeyboardInterrupt as e:\n",
    "    if input('Do you want to save the model? [y/N]: ').lower()[0] == 'y':\n",
    "        filename = datetime_str() + '.pt'\n",
    "        torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "        print(f'Model saved @ {join(exp_dir, filename)}')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing playground (Junk below here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(20, 5) + 5\n",
    "b = np.random.randn(20, 5)\n",
    "# result1 = np.empty((1, 20, 20, 5))\n",
    "# result2 = np.empty((1, 20, 20, 5))\n",
    "path = np.einsum_path('ikjk->', np.empty((20,5,20,5)), optimize='optimal')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n100 -r10\n",
    "subout = np.subtract.outer(a, b)\n",
    "result1 = np.empty((1, 20, 20, 5))\n",
    "# path = np.einsum_path('ikjk->', subout, optimize='optimal')[0]\n",
    "result1[0] = np.einsum('ikjk->', subout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n100 -r10\n",
    "result2 = np.empty((1, 20, 20, 5))\n",
    "for i in range(a.shape[0]):\n",
    "    result2[0, i] = a[i] - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_df = week1_data[(week1_data.gameId == 2018090600) & (week1_data.playId == 75) & (week1_data.frameId == 1)]\n",
    "# frame_df.values.T[...,None].shape\n",
    "\n",
    "# test = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "# len(test)\n",
    "# frame_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
