{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from os.path import join\n",
    "from utils import get_repo_dir, datetime_str, logger, jsonify, sweep\n",
    "from config.config import *\n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Add yard line feature\n",
    "* Add GPU support\n",
    "* Add hyperparameter sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial variables and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = 30  # truncate if snap-to-throw is > this. units: ds\n",
    "num_features = 10  # number of channels in input (vx, vy, etc)\n",
    "out_dir = join(get_repo_dir(), 'output')\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "exps_dir = join(out_dir, 'exps')\n",
    "os.makedirs(exps_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# week1_tracking = pd.read_csv(join(data_dir, 'week1_norm.csv'))\n",
    "# week1_coverage = pd.read_csv(join(data_dir, 'coverages_week1.csv'), dtype={'coverage': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge data\n",
    "# week1_data = pd.merge(week1_tracking, week1_coverage, how='right', on=['gameId', 'playId'])\n",
    "# week1_data['coverage_code'] = week1_data.coverage.cat.codes\n",
    "# num_classes = len(week1_data.coverage.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "weeks = range(1, 18)\n",
    "tracking_data = pd.concat([pd.read_csv(join(data_dir, f'week{week_num}_norm.csv')) for week_num in tqdm(weeks)], axis=0)\n",
    "coverage_data = pd.read_csv(join(data_dir, 'coverages_2018.csv')).dropna(subset=['coverage'])\n",
    "coverage_data.coverage.replace({'3 Seam': 'Cover 3 Zone', 'Cover 1 Double': 'Cover 1 Man'}, inplace=True)\n",
    "coverage_data = coverage_data[~coverage_data.coverage.isin(['Bracket', 'Mis'])]\n",
    "coverage_data.coverage = coverage_data.coverage.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "full_data_path = join(data_dir, 'full_data.zip')\n",
    "if not os.path.isfile(full_data_path):\n",
    "    full_data = pd.merge(tracking_data, coverage_data, how='inner', left_on=['gameId', 'playId'], right_on=['game_id', 'play_id'])\n",
    "    full_data['coverage_code'] = full_data.coverage.cat.codes\n",
    "    # save full data as zip\n",
    "    full_data.reset_index(drop=True).to_csv(full_data_path, index=False, compression={'method': 'zip', 'archive_name': 'full_data.csv'})\n",
    "else:\n",
    "    # load full data from previously saved zip\n",
    "    full_data = pd.read_csv(full_data_path)\n",
    "coverage_label_map = dict(enumerate(full_data['coverage'].cat.categories))\n",
    "num_classes = len(coverage_label_map)\n",
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tracking_data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO data aug. this is very slightly complicated by the fact that we have precomputed the vx, vy, ax, ay. will have to ask udit about how this is computed since it seems to be inconsistent with v_theta and v_mag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we don't have orientation relative to qb, so I don't think it makes sense to highlight the qb in the way the bdb winner does for rusher, since it would just be distance from qb and qb-relative speed/accel data right now, which I doubt is _extra_ useful for predicting coverage (?). so for now, we'll do 11x11 of off x def with features:\n",
    "* relative: x, y, vx, vy, ax, ay\n",
    "* absolute: vx, vy, ax, ay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    grouped = full_data.groupby(['gameId', 'playId'])\n",
    "    # TODO can change 11s to max number of off and def players in data\n",
    "    data_X = np.empty((len(grouped), max_num_frames, num_features, 11, 11), dtype=np.float32)  # (P, T, F, D, O): play, frame, feature, def, off\n",
    "    data_dims = np.empty((len(grouped), 3), dtype=np.int32) # (P, 3) contains (t, d, o): num frames, num def, num off on each play\n",
    "    data_Y = np.empty(len(grouped), dtype=np.int32)  # (P,)\n",
    "\n",
    "    valid_plays = 0\n",
    "    for (game_id, play_id), play_df in tqdm(grouped):\n",
    "        try:\n",
    "            first_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'ball_snap')].frameId.iloc[0]\n",
    "            play_end_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event.isin(['pass_forward', 'pass_shovel', 'qb_sack', 'qb_strip_sack', 'tackle']))].frameId.iloc[0]\n",
    "        except:\n",
    "            print(f'({game_id}, {play_id}) failed. events were {play_df.event.unique()}')\n",
    "            continue\n",
    "        last_frame = min(first_frame + max_num_frames - 1, play_end_frame)\n",
    "        play_df = play_df.loc[play_df.frameId.between(first_frame, last_frame)]\n",
    "\n",
    "        num_def, num_off = 0, 0\n",
    "        for frame_idx, (frame_id, frame_df) in enumerate(play_df.groupby('frameId')):\n",
    "            def_ids = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "            num_def = len(def_ids)\n",
    "            off_ids = frame_df[frame_df.team_pos == 'OFF'].index\n",
    "            num_off = len(off_ids)\n",
    "            if num_def > 11 or num_off > 11:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                break\n",
    "            if num_def < 3 or num_off < 3:\n",
    "                print(f'({game_id}, {play_id}), num_def {num_def}, num_off {num_off}')\n",
    "                break\n",
    "\n",
    "            outer_sub = np.subtract.outer(\n",
    "                frame_df.loc[off_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values,\n",
    "                frame_df.loc[def_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values\n",
    "            )\n",
    "            if np.isnan(outer_sub).any():\n",
    "                print(f'({game_id}, {play_id}), frame {frame_id} has NaNs')\n",
    "                break\n",
    "            # einsum explanation: the two i's get rid of subtraction across cols\n",
    "            # k before j reorders def before off since output dims in alph. order\n",
    "            data_X[valid_plays, frame_idx, :6, :num_def, :num_off] = np.einsum('kiji->ijk', outer_sub)\n",
    "            data_X[valid_plays, frame_idx, -4:, :num_def, :num_off] = frame_df.loc[def_ids, ['v_x', 'v_y', 'a_x', 'a_y']].values.T[...,None]\n",
    "        if num_def > 11 or num_off > 11 or num_def < 3 or num_off < 3:\n",
    "            continue\n",
    "        data_Y[valid_plays] = play_df.coverage_code.iloc[0]\n",
    "        data_dims[valid_plays] = last_frame - first_frame, num_def, num_off\n",
    "        valid_plays += 1\n",
    "    data_X = data_X[:valid_plays]\n",
    "    data_dims = data_dims[:valid_plays]\n",
    "    data_Y = data_Y[:valid_plays]\n",
    "    return data_X, data_dims, data_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_save_path = join(out_dir, 'full_data.npz')\n",
    "# NOTE: uncomment to generate data and save\n",
    "data_X, data_dims, data_Y = gen_data()\n",
    "np.savez(data_save_path, x=data_X, dims=data_dims, y=data_Y)\n",
    "# NOTE: uncomment to load from save\n",
    "# saved_data = np.load(data_save_path)\n",
    "# data_X, data_dims, data_Y = saved_data['x'], saved_data['dims'], saved_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_dims, test_dims, train_Y, test_Y = train_test_split(\n",
    "    data_X, data_dims, data_Y, test_size=0.2, random_state=12\n",
    ")\n",
    "train_dataset = TensorDataset(*map(torch.tensor, [train_X, train_dims, train_Y]))\n",
    "test_dataset = TensorDataset(*map(torch.tensor, [test_X, test_dims, test_Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoverInner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 output_dim: int,\n",
    "                 conv_h: Union[int, Tuple[int, int]]=[128, 96],\n",
    "                 linear_h: Union[int, Tuple[int, int]]=[96, 256]):\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input features\n",
    "        :param output_dim: dimension of output embedding\n",
    "        :param conv_h: number of conv channels for each conv block.\n",
    "            int or tuple of 2 ints.\n",
    "        :param linear_h: number of hidden units for each linear layer.\n",
    "            int or tuple of 2 ints.    \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if type(conv_h) is int:\n",
    "            conv_h = (conv_h, conv_h)\n",
    "        if type(linear_h) is int:\n",
    "            linear_h = (linear_h, linear_h)\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(conv_h[0])\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(conv_h[0], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h[1]),\n",
    "        )\n",
    "        self.conv_h = conv_h\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(conv_h[1], linear_h[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(linear_h[0]),\n",
    "            nn.Linear(*linear_h),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(linear_h[1]),\n",
    "            nn.Linear(linear_h[1], output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, dims):\n",
    "        # let (F', F\") and (..., F*) be conv_h and linear_h args to __init__\n",
    "        orig_shape = x.shape  # (B, T, F, D, O)\n",
    "        x = x.view(-1, *orig_shape[2:])  # (B*T, F, D, O)\n",
    "        \n",
    "        x = self.conv_block_1(x)  # (B*T, F', D, O)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F', D, O)\n",
    "        \n",
    "        # this block of code handles variable number of offensive players                \n",
    "        x_max = torch.stack([\n",
    "            F.max_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F', D)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = self.conv_block_2(x)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F\", D)\n",
    "        # this block of code handles variable number of defensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool1d(each[...,:dim[1]], kernel_size=dim[1].item()).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max.shape[2:])  # (B*T, F\")\n",
    "        \n",
    "        x = self.linear_block(x)  # (B*T, F*)\n",
    "        \n",
    "        # restore shape\n",
    "        x = x.view(*orig_shape[:2], -1)  # (B, T, F*)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCoverOuterLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_size: int,\n",
    "                 num_classes: int,\n",
    "                 num_layers: int=1,\n",
    "                 rnn_type: str='LSTM',\n",
    "                 bidirectional: bool=True):\n",
    "        \"\"\"\n",
    "        :param input_dim: input embedding dimension (per frame). same as Inner's output_dim\n",
    "        :param hidden_size: dimension of LSTM hidden state\n",
    "        :param num_classes: number of coverage classes\n",
    "        :param num_layers: number of LSTM layers\n",
    "        :param bidirectional: whether RNN is bidirectional\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_dim, hidden_size, batch_first=True,\n",
    "                               num_layers=num_layers, bidirectional=bidirectional)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_dim, hidden_size, batch_first=True,\n",
    "                               num_layers=num_layers, bidirectional=bidirectional)\n",
    "        else:\n",
    "            raise ValueError(f'RNN type \"{rnn_type}\" not recognized.')\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 1 + int(bidirectional)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.num_layers * self.num_directions * hidden_size, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # x is (B, T, F*)\n",
    "        batch_size = x.shape[0]\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, dims[:,0], batch_first=True, enforce_sorted=False)\n",
    "        x = self.rnn(x)[1][0]  # last hidden state\n",
    "        x = x.view(self.num_layers, self.num_directions, batch_size, -1)\n",
    "        x = x.permute(2, 0, 1, 3).reshape(batch_size, -1)  # (B, F^)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DeepCover(nn.Module):\n",
    "    type_class_map = {\n",
    "        'LSTM': DeepCoverOuterLSTM\n",
    "    }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 inner_args: dict,\n",
    "                 outer_args: dict\n",
    "                ):\n",
    "        assert inner_args['output_dim'] == outer_args['input_dim']\n",
    "        \n",
    "        super().__init__()\n",
    "        self.outer = DeepCoverOuter(**outer_args)\n",
    "        self.inner = DeepCoverInner(**inner_args)\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        x = self.inner(x, dims)\n",
    "        x = self.outer(x, dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loop(model, dataloader, loss_fn, optimizer=None, train=True):\n",
    "    if train:\n",
    "        assert optimizer is not None, 'Optimizer must be specified in train mode.'\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    loss = correct = 0\n",
    "    for xb, dimb, yb in dataloader:\n",
    "        xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "        y_pred = model(xb, dimb)\n",
    "        loss = loss_fn(y_pred, yb.long())\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss += loss.item() * xb.shape[0]\n",
    "        correct += (torch.max(y_pred, 1)[1] == yb).sum().item()\n",
    "    acc = correct / len(dataloader.dataset)\n",
    "    loss /= len(dataloader.dataset)\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_exp(params, exp_dir):\n",
    "    \"\"\" Pre-exp variant check \"\"\"\n",
    "    if params['num_layers'] == 1 and params['dropout'] != 0:\n",
    "        # dropout makes no difference if there's only 1 layer so don't waste runs\n",
    "        return\n",
    "    \"\"\" Logging setup \"\"\"\n",
    "    stats = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    with open(join(exp_dir, 'variant.json'), 'w') as f:\n",
    "        jsonify(params, f)    \n",
    "    f_log = open(join(exp_dir, 'train.log'), 'w')\n",
    "    f_stats = open(join(exp_dir, 'stats.csv'), 'w')\n",
    "    writer = csv.DictWriter(f_stats, fieldnames=['train_loss', 'train_acc', 'test_loss', 'test_acc'])\n",
    "    writer.writeheader()\n",
    "    \n",
    "    \"\"\" Model setup \"\"\"\n",
    "    model_params, training_params = params['model_params'], params['training_params']\n",
    "    model = DeepCover(\n",
    "        inner_args={\n",
    "            'input_channels': model_params['num_features'],\n",
    "            'output_dim': model_params['embedding_dim']\n",
    "        },\n",
    "        outer_args={\n",
    "            'input_dim': model_params['embedding_dim'],\n",
    "            'hidden_size': model_params['hidden_size'],\n",
    "            'num_classes': model_params['num_classes'],\n",
    "            'bidirectional': model_params['bidirectional'],\n",
    "            'rnn_type': model_params['rnn_type'],\n",
    "            'num_layers': model_params['num_layers']\n",
    "        }\n",
    "    ).to(device)\n",
    "    \n",
    "    \"\"\" Training setup \"\"\"\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    num_epochs = training_params['num_epochs']\n",
    "    batch_size = training_params['batch_size']\n",
    "    save_freq = training_params['save_freq']\n",
    "    scheduler_type = training_params['scheduler_type']\n",
    "    best_test_acc = 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'])\n",
    "    if scheduler_type == 'exponential':\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=np.sqrt(0.1), patience=8, min_lr=9e-5)\n",
    "    else:\n",
    "        raise ValueError(f'Scheduler type \"{scheduler_type}\" not recognized.')\n",
    "\n",
    "    try:\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            \"\"\" Training \"\"\"\n",
    "            train_acc, train_loss = run_loop(model, train_loader, loss_fn, optimizer, train=True)\n",
    "\n",
    "            \"\"\" Evaluation\"\"\"\n",
    "            test_acc, test_loss = run_loop(model, test_loader, loss_fn, train=False)\n",
    "\n",
    "            scheduler.step(test_loss)\n",
    "            \"\"\" Logging \"\"\"\n",
    "            stats['train_loss'].append(train_loss)\n",
    "            stats['test_loss'].append(test_loss)\n",
    "            stats['train_acc'].append(train_acc)\n",
    "            stats['test_acc'].append(test_acc)\n",
    "            logger(f'Epoch {epoch+1:>2}/{num_epochs} | TrLoss {train_loss:>8.5f} | TrAcc {100*train_acc:>5.2f} | TeLoss {test_loss:>8.5f} | TeAcc {100*test_acc:>5.2f}',\n",
    "                  file=f_log, log_to_file=log_to_file)\n",
    "            writer.writerow({'train_loss': train_loss, 'train_acc': train_acc, 'test_loss': test_loss, 'test_acc': test_acc})\n",
    "            if (epoch >= save_freq and (best_test_acc := max(test_acc, best_test_acc)) == test_acc) or \\\n",
    "                (epoch+1) % save_freq == 0 or epoch == num_epochs - 1: \n",
    "                # new best after halfway, or save period, or last epoch\n",
    "                filename = datetime_str() + '.pt'\n",
    "                torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "                logger(f'Model saved at {join(exp_dir, filename)}', file=f_log)\n",
    "    except KeyboardInterrupt as e:\n",
    "        if input('Do you want to save the model? [y/N]: ').lower()[0] == 'y':\n",
    "            filename = datetime_str() + '.pt'\n",
    "            torch.save(model.state_dict(), join(exp_dir, filename))\n",
    "            logger(f'Model saved at {join(exp_dir, filename)}', file=f_log, log_to_file=log_to_file)\n",
    "        raise e\n",
    "    finally:\n",
    "        f_log.close()\n",
    "        f_stats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(model_space, training_space):\n",
    "    \"\"\"\n",
    "    :param model_space: dict mapping model hyperparams to list of values to search over\n",
    "    :param training_space: dict mapping training hyperparams to list of values to search over\n",
    "    Launches experiments sweeping over possible combinations of hyperparameters, saving resulting\n",
    "    experiment directories in the returned directory path.\n",
    "    \"\"\"\n",
    "    save_dir = join(exps_dir, datetime_str())\n",
    "    for exp_id, (model_params, training_params) in enumerate(zip(sweep(model_space), sweep(training_space))):\n",
    "        params = {'model_params': model_params, 'training_params': training_params}\n",
    "        exp_dir = join(save_dir, str(exp_id).zfill(4))\n",
    "        run_exp(params, exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run sweep over hyperparameters\n",
    "model_space = dict(\n",
    "    num_features=[num_features],\n",
    "    num_classes=[num_classes],\n",
    "    embedding_dim=[16, 32, 64],  # per frame\n",
    "    hidden_size=[16, 32, 64],\n",
    "    bidirectional=[True],\n",
    "    outer_type=['LSTM'],\n",
    "    num_layers=[1, 2],\n",
    "    dropout=[0, 0.3]\n",
    ")\n",
    "training_space = dict(\n",
    "    num_epochs=[40],\n",
    "    batch_size=[64, 128],\n",
    "    save_freq=[8],\n",
    "    learning_rate=[3e-3],\n",
    "    scheduler=['plateau']\n",
    ")\n",
    "run_sweep(model_space, training_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trained on week 1\n",
    "# best_model_path = join(models_dir, '03032021_040817/03032021_042056.pt')\n",
    "# best_model_path = join(models_dir, '03042021_070643/03042021_073225.pt')\n",
    "# best_model_path = join(models_dir, '03032021_092502/03032021_100205.pt')\n",
    "# # trained on weeks 1-4\n",
    "# best_model_path = join(models_dir, '03042021_135230/03042021_155248.pt')\n",
    "# best_model_path = join(models_dir, '03042021_173200/03042021_225324.pt')\n",
    "# best_model_path = join(models_dir, '03052021_053603/03052021_145102.pt')\n",
    "with open(join(os.path.dirname(best_model_path), 'variant.json'), 'r') as f:\n",
    "    variant = json.load(f)\n",
    "best_model = DeepCover(\n",
    "    inner_args={\n",
    "        'input_channels': num_features,\n",
    "        'output_dim': variant['embedding_dim']\n",
    "    },\n",
    "    outer_type=variant.get('outer_type', 'LSTM'),  # back compat.\n",
    "    outer_args={\n",
    "        'input_dim': variant['embedding_dim'],\n",
    "        'hidden_size': variant['hidden_size'],\n",
    "        'num_classes': num_classes,\n",
    "        'bidirectional': variant['bidirectional']\n",
    "    }\n",
    ").to(device)\n",
    "best_model.load_state_dict(torch.load(best_model_path))\n",
    "best_model.eval()\n",
    "y_preds = []\n",
    "ys = []\n",
    "correct = 0\n",
    "dataloader = test_loader\n",
    "for xb, dimb, yb in dataloader:\n",
    "    xb, dimb, yb = xb.to(device), dimb.to(device), yb.to(device)\n",
    "    y_logits = best_model(xb, dimb)\n",
    "    y_pred = torch.max(y_logits, 1)[1]\n",
    "    y_preds.append(y_pred)\n",
    "    ys.append(yb)\n",
    "    correct += (y_pred == yb).sum()\n",
    "acc = correct / len(dataloader.dataset)\n",
    "print(f'Accuracy: {acc*100:.3f}%')\n",
    "y_preds = torch.cat(y_preds).flatten()\n",
    "ys = torch.cat(ys).flatten()\n",
    "cm = confusion_matrix(ys, y_preds)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
