{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "from utils import get_repo_dir\n",
    "from config.config import *\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initial variables and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_frames = 30  # truncate if snap-to-throw is > this. units: ds\n",
    "out_dir = join(get_repo_dir(), 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "week1_tracking = pd.read_csv(join(data_dir, 'week1_norm.csv'))\n",
    "week1_coverage = pd.read_csv(join(data_dir, 'coverages_week1.csv'), dtype={'coverage': 'category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge data\n",
    "week1_data = pd.merge(week1_tracking, week1_coverage, how='right', on=['gameId', 'playId'])\n",
    "week1_data['coverage_code'] = week1_data.coverage.cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO data aug. this is very slightly complicated by the fact that we have precomputed the vx, vy, ax, ay. will have to ask udit about how this is computed since it seems to be inconsistent with v_theta and v_mag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for now we don't have orientation relative to qb, so I don't think it makes sense to highlight the qb in the way the bdb winner does for rusher, since it would just be distance from qb and qb-relative speed/accel data right now, which I doubt is _extra_ useful for predicting coverage (?). so for now, we'll do 11x11 of off x def with features:\n",
    "* relative: x, y, vx, vy, ax, ay\n",
    "* absolute: vx, vy, ax, ay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate the dataset as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1028it [02:00,  8.56it/s]\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "\n",
    "grouped = week1_data.groupby(['gameId', 'playId'])\n",
    "# TODO can change 11s to max number of off and def players in data\n",
    "dataX = np.empty((len(grouped), max_num_frames, num_features, 11, 11))  # (P, T, F, D, O): play, frame, feature, def, off\n",
    "dataDims = np.empty((len(grouped), 3)) # (P, 3) contains (t, d, o): num frames, num def, num off on each play\n",
    "dataY = np.empty(len(grouped))  # (P,)\n",
    "\n",
    "valid_plays = 0\n",
    "for play_idx, (game_play, play_df) in tqdm(enumerate(grouped)):\n",
    "    if 'pass_forward' not in play_df.event.unique():\n",
    "        continue\n",
    "    first_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'ball_snap')].frameId.iloc[0]\n",
    "    play_end_frame = play_df.loc[(play_df.nflId == 0) & (play_df.event == 'pass_forward')].frameId.iloc[0]\n",
    "    last_frame = min(first_frame + max_num_frames - 1, pass_forward_frame)\n",
    "    play_df = play_df.loc[play_df.frameId.between(first_frame, last_frame)]\n",
    "    \n",
    "    num_def, num_off = 0, 0\n",
    "    for frame_idx, (frame_id, frame_df) in enumerate(play_df.groupby('frameId')):\n",
    "        def_ids = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "        num_def = len(def_ids)\n",
    "        off_ids = frame_df[frame_df.team_pos == 'OFF'].index\n",
    "        num_off = len(off_ids)\n",
    "    \n",
    "        outer_sub = np.subtract.outer(\n",
    "            frame_df.loc[off_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values,\n",
    "            frame_df.loc[def_ids, ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']].values\n",
    "        )\n",
    "        # einsum explanation: the two i's get rid of subtraction across cols\n",
    "        # k before j reorders def before off since output dims in alph. order\n",
    "        if num_def > 11 or num_off > 11:\n",
    "            breakpoint()\n",
    "        dataX[play_idx, frame_idx, :6, :num_def, :num_off] = np.einsum('kiji->ijk', outer_sub)\n",
    "        dataX[play_idx, frame_idx, -4:, :num_def, :num_off] = frame_df.loc[def_ids, ['v_x', 'v_y', 'a_x', 'a_y']].values.T[...,None]\n",
    "    dataY[play_idx] = play_df.coverage_code.iloc[0]\n",
    "    dataDims[play_idx] = last_frame - first_frame, num_def, num_off\n",
    "    valid_plays += 1\n",
    "dataX = dataX[:valid_plays]\n",
    "dataDims = dataDims[:valid_plays]\n",
    "dataY = dataY[:valid_plays]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save\n",
    "data_save_path = join(out_dir, 'week1_data')\n",
    "# np.savez(data_save_path, x=dataX, dims=dataDims, y=dataY)\n",
    "# uncomment to load from save\n",
    "saved_data = np.load(data_save_path)\n",
    "dataX, dataDims, dataY = saved_data['x'], saved_data['dims'], saved_data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make a [TensorDataset](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset) out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorX = torch.Tensor(dataX)\n",
    "tensorDims = torch.Tensor(dataDims)\n",
    "tensorY = torch.Tensor(dataY)\n",
    "dataset = TensorDataset(tensorX, tensorDims, tensorY)\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCoverInner(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 output_dim: int,\n",
    "                 conv_h: Union[int, Tuple[int, int]]=[128, 96],\n",
    "                 linear_h: Union[int, Tuple[int, int]]=96, 256)\n",
    "        \"\"\"\n",
    "        :param input_channels: number of input features\n",
    "        :param output_dim: dimension of output embedding\n",
    "        :param conv_h: number of conv channels for each conv block.\n",
    "            int or tuple of 2 ints.\n",
    "        :param linear_h: number of hidden units for each linear layer.\n",
    "            int or tuple of 2 ints.    \n",
    "        \"\"\"\n",
    "        if type(conv_h) is int:\n",
    "            conv_h = (conv_h, conv_h)\n",
    "        if type(linear_h) is int:\n",
    "            linear_h = (linear_h, linear_h)\n",
    "        self.conv_block_1 = nn.Sequential([\n",
    "            nn.Conv2d(input_channels, conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(conv_h[0], conv_h[0], kernel_size=1)\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "        self.bn1 = nn.BatchNorm1d(conv_h)\n",
    "        self.conv_block_2 = nn.Sequential([\n",
    "            nn.Conv1d(conv_h[0], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h),\n",
    "            nn.Conv1d(conv_h[1], conv_h[1], kernel_size=1)\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(conv_h),\n",
    "        ])\n",
    "        self.conv_h = conv_h\n",
    "        self.linear_block = nn.Sequential([\n",
    "            nn.Linear(conv_h[1], linear_h[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(linear_h[0]),\n",
    "            nn.Linear(*linear_h),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(linear_h[1]),\n",
    "            nn.Linear(linear_h[1], linear_h[2])\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x, dims):\n",
    "        # let (F', F\") and (..., F*) be conv_h and linear_h args to __init__\n",
    "        orig_shape = x.shape  # (B, T, F, D, O)\n",
    "        x = x.view(-1, orig_shape[2:])  # (B*T, F, D, O)\n",
    "        \n",
    "        x = self.conv_block_1(x)  # (B*T, F', D, O)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F', D, O)\n",
    "        # this block of code handles variable number of offensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool2d(each[...,:dim[2]], kernel_size=(1, dim[2])).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F', D)\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max[2:])  # (B*T, F', D)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.conv_block_2(x)\n",
    "        x = x.view(*orig_shape[:2], *x.shape[1:])  # (B, T, F\", D)\n",
    "        # this block of code handles variable number of defensive players\n",
    "        x_max = torch.stack([\n",
    "            F.max_pool1d(each[...,:dim[1]], kernel_size=dim[1]).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x_avg = torch.stack([\n",
    "            F.avg_pool1d(each[...,:dim[1]], kernel_size=dim[1]).squeeze() for each, dim in zip(x, dims)\n",
    "        ])  # (B, T, F\")\n",
    "        x = (x_max * 0.3 + x_avg * 0.7).view(-1, *x_max[2:])  # (B*T, F\")\n",
    "        \n",
    "        x = self.linear_block(x)  # (B*T, F*)\n",
    "        \n",
    "        # restore shape\n",
    "        x = x.view(*orig_shape[:2], -1)  # (B, T, F*)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepCoverOuter(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        # x is (B, T, F*)\n",
    "        pass\n",
    "    \n",
    "\n",
    "class DeepCover(nn.Module):\n",
    "    def __init__(self,\n",
    "                 inner_args,\n",
    "                 outer_args):\n",
    "        self.outer = DeepCoverOuter(**outer_args)\n",
    "        self.inner = DeepCoverInner(**inner_args)\n",
    "    \n",
    "    def forward(self, x, dims):\n",
    "        x = self.inner(x, dims)\n",
    "        x = self.outer(x, dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing playground (Junk below here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(20, 5) + 5\n",
    "b = np.random.randn(20, 5)\n",
    "# result1 = np.empty((1, 20, 20, 5))\n",
    "# result2 = np.empty((1, 20, 20, 5))\n",
    "path = np.einsum_path('ikjk->', np.empty((20,5,20,5)), optimize='optimal')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.8 µs ± 29.4 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n100 -r10\n",
    "subout = np.subtract.outer(a, b)\n",
    "result1 = np.empty((1, 20, 20, 5))\n",
    "# path = np.einsum_path('ikjk->', subout, optimize='optimal')[0]\n",
    "result1[0] = np.einsum('ikjk->', subout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.1 µs ± 26.5 µs per loop (mean ± std. dev. of 10 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n100 -r10\n",
    "result2 = np.empty((1, 20, 20, 5))\n",
    "for i in range(a.shape[0]):\n",
    "    result2[0, i] = a[i] - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 14, 1)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_df = week1_data[(week1_data.gameId == 2018090600) & (week1_data.playId == 75) & (week1_data.frameId == 1)]\n",
    "# frame_df.values.T[...,None].shape\n",
    "\n",
    "# test = frame_df[frame_df.team_pos == 'DEF'].index\n",
    "# len(test)\n",
    "# frame_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
